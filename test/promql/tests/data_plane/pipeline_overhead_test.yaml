evaluation_interval: 1m

rule_files:
  - prometheus.pipeline_alerts.yaml

tests:
  # ----- Pipeline Schedule Alerting Tests -----
  - interval: 1m
    input_series:

      # see https://prometheus.io/docs/prometheus/latest/configuration/unit_testing_rules/ for explanations of the expanding notation used for the values
      # Tekton is experiencing scheduling overhead of 100%, so it will be alerted
      - series: 'pipeline_service_schedule_overhead_percentage_sum{status="succeded",namespace="foo", source_cluster="cluster01"}'
        values: '1+1x780'
      - series: 'pipeline_service_schedule_overhead_percentage_count{status="succeded",namespace="foo", source_cluster="cluster01"}'
        values: '1+1x780'
      - series: 'pipeline_service_schedule_overhead_percentage_sum{status="succeded",namespace="bar", source_cluster="cluster01"}'
        values: '2+2x780'
      - series: 'pipeline_service_schedule_overhead_percentage_count{status="succeded",namespace="bar", source_cluster="cluster01"}'
        values: '2+2x780'
      - series: 'pipeline_service_schedule_overhead_percentage_sum{status="failed",namespace="far", source_cluster="cluster01"}'
        values: '0.3+1x780'
      - series: 'pipeline_service_schedule_overhead_percentage_count{status="failed",namespace="far", source_cluster="cluster01"}'
        values: '1+1x780'

    alert_rule_test:
      - eval_time: 13h
        alertname: HighSchedulingOverhead
        exp_alerts:
          - exp_labels:
              severity: critical
              slo: "true"
              source_cluster: cluster01
            exp_annotations:
              summary: >-
                Tekton has a scheduling overhead vs. PipelineRun durations of >5%
              description: >-
                Tekton controller on cluster cluster01 the percentage of time needed to receive PipelineRun creation
                events vs. overall PipelineRun execution time is at 100% instead of less than 5%.
              alert_team_handle: <!subteam^S03GF42RBE2>
              team: pipelines
              runbook_url: TBD

  - interval: 1m
    input_series:

      # see https://prometheus.io/docs/prometheus/latest/configuration/unit_testing_rules/ for explanations of the expanding notation used for the values
      # Tekton is experiencing scheduling overhead less than 5%, should not alert
      - series: 'pipeline_service_schedule_overhead_percentage_sum{status="succeded",namespace="boo", source_cluster="cluster01"}'
        values: '0.5x780'
      - series: 'pipeline_service_schedule_overhead_percentage_count{status="succeded",namespace="boo", source_cluster="cluster01"}'
        values: '1x780'
      - series: 'pipeline_service_schedule_overhead_percentage_sum{status="succeded",namespace="far", source_cluster="cluster01"}'
        values: '300+300x780'
      - series: 'pipeline_service_schedule_overhead_percentage_count{status="succeded",namespace="far", source_cluster="cluster01"}'
        values: '600+600x780'
      - series: 'pipeline_service_schedule_overhead_percentage_sum{status="failed",namespace="far", source_cluster="cluster01"}'
        values: '300+300x780'
      - series: 'pipeline_service_schedule_overhead_percentage_count{status="failed",namespace="far", source_cluster="cluster01"}'
        values: '10000+10000x780'

    alert_rule_test:
      - eval_time: 13h
        alertname: HighSchedulingOverhead
        exp_alerts:
          - exp_labels:
              severity: critical
              slo: "true"
              source_cluster: cluster01
            exp_annotations:
              summary: >-
                Tekton has a scheduling overhead vs. PipelineRun durations of >5%
              description: >-
                Tekton controller on cluster cluster01 the percentage of time needed to receive PipelineRun creation
                events vs. overall PipelineRun execution time is at 50% instead of less than 5%.
              alert_team_handle: <!subteam^S03GF42RBE2>
              team: pipelines
              runbook_url: TBD

  - interval: 1m
    input_series:

      # see https://prometheus.io/docs/prometheus/latest/configuration/unit_testing_rules/ for explanations of the expanding notation used for the values
      # Tekton is experiencing scheduling overhead less than 5%, should not alert
      - series: 'pipeline_service_schedule_overhead_percentage_sum{status="succeded",namespace="boo", source_cluster="cluster01"}'
        values: '0.03x780'
      - series: 'pipeline_service_schedule_overhead_percentage_count{status="succeded",namespace="boo", source_cluster="cluster01"}'
        values: '1x780'
      - series: 'pipeline_service_schedule_overhead_percentage_sum{status="succeded",namespace="far", source_cluster="cluster01"}'
        values: '300+300x780'
      - series: 'pipeline_service_schedule_overhead_percentage_count{status="succeded",namespace="far", source_cluster="cluster01"}'
        values: '10000+10000x780'
      - series: 'pipeline_service_schedule_overhead_percentage_sum{status="failed",namespace="foo", source_cluster="cluster01"}'
        values: '1+1x780'
      - series: 'pipeline_service_schedule_overhead_percentage_count{status="failed",namespace="foo", source_cluster="cluster01"}'
        values: '1+1x780'

    alert_rule_test:
      - eval_time: 13h
        alertname: HighSchedulingOverhead

  # ----- Pipeline Execution Alerting Tests -----
  - interval: 1m
    input_series:

      # see https://prometheus.io/docs/prometheus/latest/configuration/unit_testing_rules/ for explanations of the expanding notation used for the values
      # Tekton is experiencing execution overhead of 100%, so it will be alerted
      - series: 'pipeline_service_execution_overhead_percentage_sum{status="succeded",namespace="foo", source_cluster="cluster01"}'
        values: '1+1x780'
      - series: 'pipeline_service_execution_overhead_percentage_count{status="succeded",namespace="foo", source_cluster="cluster01"}'
        values: '1+1x780'
      - series: 'pipeline_service_execution_overhead_percentage_sum{status="succeded",namespace="bar", source_cluster="cluster01"}'
        values: '2+2x780'
      - series: 'pipeline_service_execution_overhead_percentage_count{status="succeded",namespace="bar", source_cluster="cluster01"}'
        values: '2+2x780'
      - series: 'pipeline_service_execution_overhead_percentage_sum{status="failed",namespace="far", source_cluster="cluster01"}'
        values: '300+300x780'
      - series: 'pipeline_service_execution_overhead_percentage_count{status="failed",namespace="far", source_cluster="cluster01"}'
        values: '10000+10000x780'

    alert_rule_test:
      - eval_time: 13h
        alertname: HighExecutionOverhead
        exp_alerts:
          - exp_labels:
              severity: critical
              slo: "true"
              source_cluster: cluster01
            exp_annotations:
              summary: >-
                Tekton has a execution overhead vs. PipelineRun durations of >5%
              description: >-
                Tekton controller on cluster cluster01 the percentage of the time needed to create
                underlying TaskRuns vs. overall PipelineRun execution time is at 100% instead of less than 5%.
              alert_team_handle: <!subteam^S03GF42RBE2>
              team: pipelines
              runbook_url: TBD

  - interval: 1m
    input_series:

      # see https://prometheus.io/docs/prometheus/latest/configuration/unit_testing_rules/ for explanations of the expanding notation used for the values
      # Tekton is experiencing scheduling overhead less than 5%, should not alert
      - series: 'pipeline_service_execution_overhead_percentage_sum{status="succeded",namespace="foo", source_cluster="cluster01"}'
        values: '0.05263x780'
      - series: 'pipeline_service_execution_overhead_percentage_count{status="succeded",namespace="foo", source_cluster="cluster01"}'
        values: '1x780'
      - series: 'pipeline_service_execution_overhead_percentage_sum{status="succeded",namespace="foo", source_cluster="cluster01"}'
        values: '1+1x780'
      - series: 'pipeline_service_execution_overhead_percentage_count{status="succeded",namespace="foo", source_cluster="cluster01"}'
        values: '19+19x780'
      - series: 'pipeline_service_execution_overhead_percentage_sum{status="succeded",namespace="far", source_cluster="cluster01"}'
        values: '2+2x780'
      - series: 'pipeline_service_execution_overhead_percentage_count{status="succeded",namespace="far", source_cluster="cluster01"}'
        values: '38+38x780'
      - series: 'pipeline_service_execution_overhead_percentage_sum{status="failed",namespace="foo", source_cluster="cluster01"}'
        values: '300+300x780'
      - series: 'pipeline_service_execution_overhead_percentage_count{status="failed",namespace="foo", source_cluster="cluster01"}'
        values: '10000+10000x780'

    alert_rule_test:
      - eval_time: 13h
        alertname: HighExecutionOverhead
        exp_alerts:
          - exp_labels:
              severity: critical
              slo: "true"
              source_cluster: cluster01
            exp_annotations:
              summary: >-
                Tekton has a execution overhead vs. PipelineRun durations of >5%
              description: >-
                Tekton controller on cluster cluster01 the percentage of the time needed to create
                underlying TaskRuns vs. overall PipelineRun execution time is at 5.263% instead of less than 5%.
              alert_team_handle: <!subteam^S03GF42RBE2>
              team: pipelines
              runbook_url: TBD

  - interval: 1m
    input_series:

      # see https://prometheus.io/docs/prometheus/latest/configuration/unit_testing_rules/ for explanations of the expanding notation used for the values
      # Tekton is experiencing execution overhead less than 5%, should not alert
      - series: 'pipeline_service_execution_overhead_percentage_sum{status="succeded",namespace="foo", source_cluster="cluster01"}'
        values: '0.03x780'
      - series: 'pipeline_service_execution_overhead_percentage_count{status="succeded",namespace="foo", source_cluster="cluster01"}'
        values: '1x780'
      - series: 'pipeline_service_execution_overhead_percentage_sum{status="succeded",namespace="foo", source_cluster="cluster01"}'
        values: '300+300x780'
      - series: 'pipeline_service_execution_overhead_percentage_count{status="succeded",namespace="foo", source_cluster="cluster01"}'
        values: '10000+10000x780'
      - series: 'pipeline_service_execution_overhead_percentage_sum{status="succeded",namespace="bar", source_cluster="cluster01"}'
        values: '300+300x780'
      - series: 'pipeline_service_execution_overhead_percentage_count{status="succeded",namespace="bar", source_cluster="cluster01"}'
        values: '10000+10000x780'
      - series: 'pipeline_service_execution_overhead_percentage_sum{status="failed",namespace="bar", source_cluster="cluster01"}'
        values: '2+2x780'
      - series: 'pipeline_service_execution_overhead_percentage_count{status="failed",namespace="bar", source_cluster="cluster01"}'
        values: '2+2x780'

    alert_rule_test:
      - eval_time: 13h
        alertname: HighExecutionOverhead
